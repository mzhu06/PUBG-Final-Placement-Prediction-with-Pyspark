# -*- coding: utf-8 -*-
"""
Created on Sun Mar 24 14:43:57 2019

@author: Qi Wang, Mingyuan Zhu
"""

from pyspark.sql import SparkSession
import numpy as np

spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext

import pyspark
from pyspark.ml import feature, regression, Pipeline, classification,evaluation
from pyspark.sql import functions as fn, Row
from pyspark import sql
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.classification import LogisticRegression,GBTClassifier
from pyspark.ml.feature import StringIndexer, VectorIndexer

import matplotlib.pyplot as plt
import pandas as pd


##############################################################################
##########       Spark creat and transform data             #################
##############################################################################
## Spark Method ##
# Full_data = spark.read.csv('test_V2.csv', sep=',', inferSchema=True, header=True)
Full_data = spark.read.csv('train_V2.csv', sep=',', inferSchema=True, header=True)
Full_data.printSchema
# seperate match type
##############################################################################
#  combine the original matchtype to only three types : solo, duo squad
##############################################################################
update_fun = (fn.when(fn.col('matchType').contains('solo'), 'solo').when(fn.col('matchType').contains('duo' or 'crash'), 'duo')
                .otherwise('squad'))
Full_data = Full_data.withColumn('matchType', update_fun)
##############################################################################
###### collect solo match
##############################################################################
###    Feature engineering
#  combined rideDistance, walkDistance, swimdistance as total Distance
#  create killpercentile feature = killplace/maxplace
New_df_solo = Full_data.filter(Full_data.matchType == 'solo')
New_df_solo = New_df_solo.withColumn('totalDistance',fn.col('rideDistance') + fn.col('walkDistance')+ fn.col('swimDistance'))
New_df_solo = New_df_solo.withColumn('killpercentile',fn.col('killPlace')/fn.col('maxPlace'))
columns = ['Id','groupId','matchId','matchType','rideDistance','walkDistance','swimDistance','rankPoints','killPoints','winPoints','killPlace','maxPlace']
New_df_solo = New_df_solo.select([col for col in New_df_solo.columns if col not in columns])
pj_sp_df_solo = New_df_solo.sample(withReplacement=False, fraction=0.01, seed=3)
##############################################################################
###### collect team match
# combined all the personal features into teamfeatures
########
New_df_team = Full_data.filter(Full_data.matchType != 'solo')
New_df_team  = Full_data.select('*').groupby('groupId').agg(fn.sum('damageDealt').alias('total_team_damage'),
                      fn.sum('kills').alias('total_team_kills'),
                      fn.sum('killPoints').alias('team_kill_points'),
                      fn.avg('killPlace').alias('team_kill_rank'),
                      fn.avg('rankPoints').alias('team_normal_rank'),
                      fn.sum('revives').alias('team_revives'),
                      fn.sum('boosts').alias('team_boosts'),
                      fn.sum('assists').alias('total_assists'),
                      fn.sum('DBNOs').alias('team_DBNOs'),
                      fn.sum(Full_data.rideDistance + Full_data.walkDistance + Full_data.swimDistance).alias('totalDistance'))

New_df_team  = New_df_team.join(Full_data,New_df_team .groupId == Full_data.groupId)
New_df_team = New_df_team.withColumn('teamkillpercentile',fn.col('team_kill_rank')/fn.col('maxPlace'))


##
columns = ['Id','groupId','matchId', 'roadKills','numGroups','rideDistance','walkDistance','swimDistance','team_kill_rank','maxPlace','kills','killPoints','killPlace','rankPoints','revives','boosts','assists','DBNOs','damageDealt','team_normal_rank']
New_df_team = New_df_team.select([col for col in New_df_team.columns if col not in columns])

################################################################################
pj_sp_df_team = New_df_team.sample(withReplacement=False, fraction=0.01, seed=3)
##
#withColumn('solo',(fn.col('matchType') == 'solo').cast('int')).\
pj_sp_df_team = pj_sp_df_team.\
                    withColumn('duo',(fn.col('matchType') == 'duo').cast('int')).\
                    withColumn('squad',(fn.col('matchType') == 'squad').cast('int'))
pj_sp_df_team = pj_sp_df_team.drop('matchType')
pj_sp_df_team.show(5)
###############################################################################
##Full Data if needed
#pj_sp_df_solo = New_df_solo
#pj_sp_df_team = New_df_team.\
#                    withColumn('duo',(fn.col('matchType') == 'duo').cast('int')).\
#                    withColumn('squad',(fn.col('matchType') == 'squad').cast('int'))
#pj_sp_df_team = pj_sp_df_team.drop('matchType')

#####################################################################
###############         Model Part       #############           
#####################################################################
#### Build a PCA Class
class PCA:
    def __init__(self,df):
        self.df = df
        self.inputcol = self.df.columns
        self.inputcol.remove('winPlacePerc')
    
    def Pipe_line(self,k):
         self.pipeline_PCA = Pipeline(stages=[
                            feature.VectorAssembler(inputCols=self.inputcol,outputCol='features'),
                            feature.StandardScaler(withMean=True,inputCol='features', outputCol='zfeatures'),
                            feature.PCA(k=k, inputCol='zfeatures', outputCol='loadings')
                            ]).fit(self.df)
         return self.pipeline_PCA
     
    def PC(self):
        self.principal_components = self.pipeline_PCA.stages[-1].pc.toArray()
        self.pca_model = self.pipeline_PCA.stages[-1]
        self.explainedVariance = self.pca_model.explainedVariance
        return {'PC':self.principal_components,"Model":self.pca_model,"EV":self.explainedVariance}
    
    def choose(self,th = 0.98):
        for i in range(0,1000):
             if self.explainedVariance[i] < 0.01:
                break
        print ('best k = ', i-1)
        self.sumEV = 0
        for i in range(0,1000):
             self.sumEV += self.explainedVariance[i]
             if self.sumEV > th:
                    break
        print ('best k = ', i-1)     
        plt.plot(np.cumsum(self.explainedVariance))
        plt.xlabel('number of components')
        plt.ylabel('cumulative explained variance')
    
    def Print_first2(self):
        pc1_pd =pd.DataFrame({'Feature': self.inputcol, 'abs_loading': abs(self.principal_components[:,0])}).sort_values('abs_loading',ascending=False)
        pc2_pd =pd.DataFrame({'Feature': self.inputcol, 'abs_loading': abs(self.principal_components[:,1])}).sort_values('abs_loading',ascending=False)
        return {'pc1':pc1_pd,'pc2':pc2_pd}
    
    def reduced_dm(self,n):
        PCA_choosed = self.principal_components[:,:n]
        tpdf = self.df.drop('winPlacePerc')
        sp_np = tpdf.toPandas().values
        pc_df_solo = np.dot(sp_np,PCA_choosed)
        pc_df_solo =pd.DataFrame(pc_df_solo)
        return pc_df_solo
        
##########################################################      
##########    PCA  (solo)      #################### 
solo_model = PCA(pj_sp_df_solo)
solo_model.Pipe_line(18)
solo_model.PC()
solo_model.choose()
##########         PC Results first two dimension
pc1_pd = solo_model.Print_first2()['pc1']
pc2_pd = solo_model.Print_first2()['pc2']
##############      Reduced Dimension      #############################
pc_df_solo = solo_model.reduced_dm(13)
pc_df_solo.describe()
##########################################################  

##########    PCA  (team)      #################### 
team_model = PCA(pj_sp_df_team)
team_model.Pipe_line(18)
team_model.PC()
team_model.choose()
##########         PC Results first two dimension
pc1_pd = team_model.Print_first2()['pc1']
pc2_pd = team_model.Print_first2()['pc2']
##############      Reduced Dimension      #############################
pc_df_team = team_model.reduced_dm(15)
pc_df_team.describe()
########################################################## 
########################################################## 
## Linear with Origin
# RMSE Function
rmse = fn.sqrt(fn.mean((fn.col('winPlacePerc')-fn.col('prediction'))**2)).alias("rmse")   
def rmseLr(alpha, beta, fit_df, transform_df, test_df,maxiter = 100):
    lr =  LinearRegression().\
        setLabelCol('winPlacePerc').\
        setFeaturesCol('scaledFeatures').\
        setRegParam(beta).\
        setMaxIter(maxiter).\
        setElasticNetParam(alpha)

    pipe_Lr_og = Pipeline(stages = [
      feature.VectorAssembler(inputCols = inputcol ,outputCol = 'feature'),
      feature.StandardScaler(withMean=True,inputCol="feature", outputCol="scaledFeatures"),
      lr]).fit(fit_df)

    coeff = pipe_Lr_og.stages[2].coefficients.toArray()
    coeffs_df = pd.DataFrame({'Features': inputcol, 'Coeffs': coeff})
    coeffs_df = coeffs_df.sort_values('Coeffs', ascending=False)
    
    ## rmse of validationdata
    print('RMSE of Validation Data Set')
    rmse3_df = pipe_Lr_og.transform(transform_df).select(rmse)
    
    ## Summary of testing data
    train_summary = pipe_Lr_og.stages[-1].summary
    print("Training RMSE: %f" % train_summary.rootMeanSquaredError)
    print("Trainingr2: %f" % train_summary.r2)
    
    lr_predictions = pipe_Lr_og.transform(test_df)
    lr_evaluator = RegressionEvaluator(predictionCol="prediction", \
                 labelCol="winPlacePerc",metricName="r2")
    
    print("R Squared (R2) on test data = %g" % lr_evaluator.evaluate(lr_predictions))
    
    return {'rmse':rmse3_df.show(),'pipe_model':pipe_Lr_og,'Train_su': train_summary,'Test_model':lr_predictions,'coeff':coeffs_df}


############################################################################3
def runModel(model,training_df, validation_df, testing_df):
    n = 0
    for i in np.arange(0,1.1,0.1):
       if i == 0 or i == 1:
          for j in np.arange(0,0.2,0.1):
            n+=1
            print('Model:',n,'Alpha:',i,"Lambda: ",j)
            model(i,j,training_df, validation_df, testing_df)
       else:
           j = 0.1
           n+=1
           print('Model:',n,'Alpha:',i,"Lambda: ",j)
           model(i,j,training_df, validation_df, testing_df)

############################################################################3
##########     Linear Model of Team  Original Data  ############
training_df, validation_df, testing_df = pj_sp_df_team.randomSplit([0.6, 0.3, 0.1], seed=0)
inputcol = pj_sp_df_team.columns
inputcol.remove('winPlacePerc')
##########       Model  Out put
runModel(rmseLr,training_df, validation_df, testing_df)
############ Model is the best             ########################
#         Model: 1 Alpha: 0.0 Lambda:  0.0
#             RMSE of Validation Data Set
#             Training RMSE: 0.129407
#             Trainingr2: 0.822858
#             R Squared (R2) on test data = 0.824897
#             +-------------------+
#             |               rmse|
#             +-------------------+
#             |0.12931329250256463|
#             +-------------------+
lr_t = rmseLr(0,0,training_df, validation_df, testing_df)
lr_t['coeff']
################################################################
        
##########     Linear Model of solo  Original Data  ############
training_df, validation_df, testing_df = pj_sp_df_solo.randomSplit([0.6, 0.3, 0.1], seed=0)
inputcol = pj_sp_df_solo.columns
inputcol.remove('winPlacePerc')
##########       Model  Out put
runModel(rmseLr,training_df, validation_df, testing_df)
############ Model is the best             ########################
#         Model: 1 Alpha: 0.0 Lambda:  0.0
#           RMSE of Validation Data Set
#           Training RMSE: 0.104621
#           Trainingr2: 0.877490
#           R Squared (R2) on test data = 0.87965
#           +-------------------+
#           |               rmse|
#           +-------------------+
#           |0.10608384409927706|
#           +-------------------+

lr_s = rmseLr(0,0,training_df, validation_df, testing_df)
lr_s['coeff']
################################################################
#############       PCA Linear Model     ###################
pc_sp_df_team = spark.createDataFrame(pc_df_team)
#pc_sp_df_team.withColumn('winPlacePerc',pj_sp_df_team['winPlacePerc'])
pc_sp_df_solo = spark.createDataFrame(pc_df_solo)
#############################################################
#Combine pc dataframe with column 'winPlacePerc'
def combinedf(pc_df, pj_sp_df):
    pc_df1 =  spark.createDataFrame(pc_df)
    pc_df1=pc_df1.withColumn('index',fn.monotonically_increasing_id())
    pj_sp_df1 = pj_sp_df.select('winPlacePerc').withColumn('index',fn.monotonically_increasing_id())
    join_df = pc_df1.join(pj_sp_df1, on=['index'])
    return(join_df)
##############################################################
##########     Linear Model of Solo  PCA Data  ############
##########       Model  Out put
join_pc_df_solo = combinedf(pc_df_solo,pj_sp_df_solo)
inputcol = list(join_pc_df_solo.columns)
inputcol.remove('winPlacePerc')
inputcol.remove('index')
training_df, validation_df, testing_df = join_pc_df_solo.randomSplit([0.6, 0.3, 0.1], seed=0)
##############################################################
runModel(rmseLr,training_df, validation_df, testing_df)     
##############################################################
##########     Linear Model of Team  PCA Data  ############
##########       Model  Out put
join_pc_df_team = combinedf(pc_df_team,pj_sp_df_team)
inputcol = list(join_pc_df_team.columns)
inputcol.remove('winPlacePerc')
inputcol.remove('index')
training_df, validation_df, testing_df = join_pc_df_team.randomSplit([0.6, 0.3, 0.1], seed=0)
##########       Model  Out put
runModel(rmseLr,training_df, validation_df, testing_df)


##############################################################
#      Logit     model
##############################################################
#split the data set into ten categories in 
# bucket
splits = [ float("-inf"), 0.1, 0.2, 0.3,0.4,0.5,0.6,0.7,0.8,0.9, float('Inf') ]
from pyspark.ml.feature import Bucketizer
bucketizer = Bucketizer(splits=splits ,inputCol='winPlacePerc', outputCol='rank')
pj_lg_df_solo = bucketizer.setHandleInvalid("keep").transform(pj_sp_df_solo).drop('winPlacePerc')
pj_lg_df_team = bucketizer.setHandleInvalid("keep").transform(pj_sp_df_team).drop('winPlacePerc')

from pyspark.ml.evaluation import MulticlassClassificationEvaluator
#####################################################################
#### Ordinarl logistic regression Model
training_df, validation_df, testing_df = pj_lg_df_team.randomSplit([0.6, 0.3, 0.1], seed=0)
def logitReg(reg,elastic,training,validate,test):
    inputcol = training_df.columns
    inputcol.remove('rank')
    logitReg = LogisticRegression().\
        setMaxIter(100).\
        setLabelCol('rank').\
        setFeaturesCol('scaledFeatures').\
        setRegParam(reg).\
        setElasticNetParam(elastic)

    pipe_log=Pipeline(stages=[
        feature.VectorAssembler(inputCols=inputcol, outputCol='features'),
        feature.StandardScaler(withMean=True, inputCol='features', outputCol='scaledFeatures'),
        logitReg
        ]).fit(training)
#    logSummary = pipe_log.stages[-1].summary
#    accuracy = logSummary.accuracy
#    falsePositiveRate = logSummary.weightedFalsePositiveRate
#    truePositiveRate = logSummary.weightedTruePositiveRate
#    fMeasure = logSummary.weightedFMeasure()
#    precision = logSummary.weightedPrecision
#    recall = logSummary.weightedRecall
#    print("Accuracy: %s\nFPR: %s\nTPR: %s\nF-measure: %s\nPrecision: %s\nRecall: %s"   % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))
    predictions = pipe_log.transform(validate)
    evaluator = MulticlassClassificationEvaluator(labelCol="rank", predictionCol="prediction", metricName="accuracy")
    accuracy = evaluator.evaluate(predictions)
    print("Validate Error = %g" % (1.0 - accuracy),"Accuracy = %g" % (accuracy))
    predictions = pipe_log.transform(test)
    accuracy = evaluator.evaluate(predictions)
    print("Test Error = %g" % (1.0 - accuracy),"Accuracy = %g" % (accuracy))
##################################################

###################  
runModel(logitReg,training_df, validation_df, testing_df)
#####################################################################
training_df, validation_df, testing_df = pj_lg_df_solo.randomSplit([0.6, 0.3, 0.1], seed=0)
###################  
runModel(logitReg,training_df, validation_df, testing_df)

####################################################################################
training_df, validation_df, testing_df = pj_sp_df_team.randomSplit([0.6, 0.3, 0.1], seed=0)
inputcol = pj_sp_df_team.columns
inputcol.remove('winPlacePerc')

def gbr(training_df,validation_df, testing_df,md=3,mi=100):
    gbr = regression.GBTRegressor(seed=0,maxDepth=md).\
          setMaxIter(mi).\
          setLabelCol('winPlacePerc').\
          setFeaturesCol('features')
    
    va =  feature.VectorAssembler(inputCols=inputcol, outputCol='features')   

    pipe_gbr=Pipeline(stages=[va, gbr]).fit(training_df)

    evaluator = evaluation.RegressionEvaluator(labelCol='winPlacePerc', metricName='r2')
    evaluator2 = evaluation.RegressionEvaluator(labelCol='winPlacePerc', predictionCol="prediction", metricName="rmse")

    predictions = pipe_gbr.transform(validation_df)
    R2_gbr = evaluator.evaluate(predictions)
    rmse = evaluator2.evaluate(predictions)
    print('mi:',mi,'md:',md)
    print("Performance of GBR(validate) R2: ", R2_gbr)
    print("---------------------------------------")
    print("Performance of GBR(validate) RMSE: ", rmse)
    
    return pipe_gbr

####################################################################################   
for mi in range(30,100,10):
    for md in range(3,10):
        gbr(training_df,validation_df, testing_df,md,mi)

#### Best Model for team data
### maxdepth = 6 ,ax iteration = 100
gbr_team = gbr(training_df,validation_df, testing_df,6,100 )
pd.DataFrame(list(zip(inputcol, gbr_team.stages[-1].featureImportances.toArray())),
            columns = ['column', 'weight']).sort_values('weight')

####################     solo model   ############################################
training_df, validation_df, testing_df = pj_sp_df_solo.randomSplit([0.6, 0.3, 0.1], seed=0)
inputcol = pj_sp_df_solo.columns
inputcol.remove('winPlacePerc')

####################################################################################
for mi in range(30,100,10):
    for md in range(3,10):
        gbr(training_df,validation_df, testing_df,md,mi)

####################################################################################    
gbr_solo = gbr(training_df,validation_df, testing_df,8,100)
pd.DataFrame(list(zip(inputcol, gbr_solo.stages[-1].featureImportances.toArray())),
            columns = ['column', 'weight']).sort_values('weight')

#######################################################
##########     Randrom Forest of Solo Data  ###########
rf_solo_df = pj_sp_df_solo
# rf_solo_df.withColumnRenamed('winPlacePerc', 'label')
inputcols = rf_solo_df.columns
inputcols.remove('winPlacePerc')

#split data into train, validation and test
training_df, validation_df, testing_df = rf_solo_df.randomSplit([0.6, 0.3, 0.1], seed=0)

#stages
def modelRF(training_df,validation_df, testing_df,md=3,mi=100):
    va = feature.VectorAssembler(inputCols=inputcols, outputCol='features')
    rf = regression.RandomForestRegressor(seed = 0,maxDepth=md,numTrees=mi).\
          setLabelCol('winPlacePerc').setFeaturesCol('features')
    # Chain indexer and forest in a Pipeline
    rf_pipeline = Pipeline(stages=[va, rf])
    # Train model.  This also runs the indexer.
    rf_model = rf_pipeline.fit(training_df)
    #Test on validation dataset
    predictions = rf_model.transform(validation_df)
    #select example rows to display
    #predictions.select('prediction', 'winPlacePerc','features').show(5)
    # validation error
    # Select (prediction, true label) and compute test error
    evaluator = evaluation.RegressionEvaluator(labelCol='winPlacePerc', predictionCol="prediction", metricName="rmse")
    rmse = evaluator.evaluate(predictions)
    print('mi:',mi,'md:',md)
    print("Root Mean Squared Error (RMSE) on validation data = %g" % rmse)
    return rf_model

#################################################################
for mi in range(30,100,10):
    for md in range(3,10):
        modelRF(training_df,validation_df, testing_df,md,mi)

modelRF(training_df,validation_df, testing_df)
#################################################################
##########     Randrom Forest of Team  Original Data  ###########

rf_team_df = pj_sp_df_team
#rf_team_df.withColumnRenamed('winPlacePerc', 'label')
inputcols = rf_team_df.columns
inputcols.remove('winPlacePerc')

#split data into train, validation and test
training_df, validation_df, testing_df = rf_team_df.randomSplit([0.6, 0.3, 0.1], seed=0)
#from pyspark.ml.feature import VectorAssembler

for mi in range(30,100,10):
    for md in range(3,10):
        modelRF(training_df,validation_df, testing_df,md,mi)

modelRF(training_df,validation_df, testing_df)
####################################################################################
########   MLP Model for solo data
training_df, validation_df, testing_df = pj_lg_df_solo.randomSplit([0.6, 0.3, 0.1], seed=0)
inputcols =  pj_lg_df_solo.columns
inputcols.remove('rank')
mlp = classification.MultilayerPerceptronClassifier(seed=0).\
    setStepSize(0.2).\
    setMaxIter(100).\
    setLabelCol('rank').\
    setFeaturesCol('features').\
    setLayers([18,30,15, 10])
    
va =  feature.VectorAssembler(inputCols=inputcols, outputCol='features')   


pipe_mlp=Pipeline(stages=[va, mlp]).fit(training_df)

evaluator = evaluation.MulticlassClassificationEvaluator(labelCol='rank', predictionCol="prediction",metricName="accuracy")

predictions = pipe_mlp.transform(validation_df)
accuracy = evaluator.evaluate(predictions)
print('Accuracy:' ,accuracy)

####################################################################################
########   MLP Model for team  data
training_df, validation_df, testing_df = pj_lg_df_team.randomSplit([0.6, 0.3, 0.1], seed=0)
inputcols =  pj_lg_df_team.columns
inputcols.remove('rank')
mlp = classification.MultilayerPerceptronClassifier(seed=0).\
    setStepSize(0.2).\
    setMaxIter(100).\
    setLabelCol('rank').\
    setFeaturesCol('features').\
    setLayers([20,20, 10])
    
va =  feature.VectorAssembler(inputCols=inputcols, outputCol='features')   


pipe_mlp=Pipeline(stages=[va, mlp]).fit(training_df)

evaluator = evaluation.MulticlassClassificationEvaluator(labelCol='rank', predictionCol="prediction",metricName="accuracy")

predictions = pipe_mlp.transform(validation_df)
accuracy = evaluator.evaluate(predictions)
print('Accuracy:' ,accuracy)

#######################################################################################
#paramGrid = ParamGridBuilder() \
#    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \
#    .addGrid(lr.regParam, [0.1, 0.01]) \
#    .build()
#
#crossval = CrossValidator(estimator=pipeline,
#                          estimatorParamMaps=paramGrid,
#                          evaluator=BinaryClassificationEvaluator(),
#                          numFolds=2)  # use 3+ folds in practice

####################################################################################################################
 #############################                        Prediction                       ############################# 
####################################################################################################################
#############################       Spark creat  Test and transform data             ###############################
####################################################################################################################
## Spark Method ##
# Full_data = spark.read.csv('test_V2.csv', sep=',', inferSchema=True, header=True)
Full_test_data = spark.read.csv('test_V2.csv', sep=',', inferSchema=True, header=True)
# seperate match type
update_fun = (fn.when(fn.col('matchType').contains('solo'), 'solo').when(fn.col('matchType').contains('duo' or 'crash'), 'duo')
                .otherwise('squad'))
Full_test_data = Full_test_data.withColumn('matchType', update_fun)
################        Creat solo input Datafram      ###############################
###### collect solo match
New_df_solo = Full_test_data.filter(Full_test_data.matchType == 'solo')
New_df_solo = New_df_solo.withColumn('totalDistance',fn.col('rideDistance') + fn.col('walkDistance')+ fn.col('swimDistance'))
New_df_solo = New_df_solo.withColumn('killpercentile',fn.col('killPlace')/fn.col('maxPlace'))
columns = ['groupId','matchId','matchType','rideDistance','walkDistance','swimDistance','rankPoints','killPoints','winPoints','killPlace','maxPlace']
New_df_solo = New_df_solo.select([col for col in New_df_solo.columns if col not in columns])
#New_df_solo = New_df_solo.select('*').fn.sum(New_df_solo.rideDistance + New_df_solo.walkDistance + New_df_solo.swimDistance).alias('totalDistance')
test_sp_df_solo = New_df_solo
#test_sp_df_solo = New_df_solo.sample(withReplacement=False, fraction=0.1, seed=3)
################        Creat team input Datafram       ##############################
###### collect team match
New_df_team = Full_test_data.filter(Full_test_data.matchType != 'solo')
New_df_team  = New_df_team.select('*').groupby('groupId').agg(fn.sum('damageDealt').alias('total_team_damage'),
                      fn.sum('kills').alias('total_team_kills'),
                      fn.sum('killPoints').alias('team_kill_points'),
                      fn.avg('killPlace').alias('team_kill_rank'),
                      fn.avg('rankPoints').alias('team_normal_rank'),
                      fn.sum('revives').alias('team_revives'),
                      fn.sum('boosts').alias('team_boosts'),
                      fn.sum('assists').alias('total_assists'),
                      fn.sum('DBNOs').alias('team_DBNOs'),
                      fn.sum(Full_test_data.rideDistance + Full_test_data.walkDistance + Full_test_data.swimDistance).alias('totalDistance'))
New_df_team  = New_df_team.join(Full_test_data,New_df_team .groupId == Full_test_data.groupId)
New_df_team = New_df_team.withColumn('teamkillpercentile',fn.col('team_kill_rank')/fn.col('maxPlace'))

##
columns = ['groupId','matchId', 'roadKills','numGroups','rideDistance','walkDistance','swimDistance','team_kill_rank','maxPlace','kills','killPoints','killPlace','rankPoints','revives','boosts','assists','DBNOs','damageDealt','team_normal_rank']
New_df_team = New_df_team.select([col for col in New_df_team.columns if col not in columns])


################        Creat team Result Datafram       ##################################
test_sp_df_team = New_df_team
test_sp_df_team = test_sp_df_team.\
                    withColumn('duo',(fn.col('matchType') == 'duo').cast('int')).\
                    withColumn('squad',(fn.col('matchType') == 'squad').cast('int'))
test_sp_df_team = test_sp_df_team.drop('matchType')
test_sp_df_team.show(5)
####################################################################################
id_team = test_sp_df_team.select('Id').toPandas()
#inputcol.remove('winPlacePerc')
test_sp_df_team=test_sp_df_team.drop('Id')
predict_team = gbr_team.transform(test_sp_df_team)
result_pre = predict_team.select('prediction').toPandas()
result_team = pd.concat([id_team, result_pre], axis=1)
###############       Creat solo Result Datafram       ####################################
id_solo= test_sp_df_solo.select('Id').toPandas()
#inputcol.remove('winPlacePerc')
test_sp_df_solo=test_sp_df_solo.drop('Id')
predict_solo = gbr_solo.transform(test_sp_df_solo)
result_pre = predict_solo.select('prediction').toPandas()
result_solo = pd.concat([id_solo, result_pre], axis=1)
##################################################################################
result = pd.concat([result_solo, result_team])
id_test = Full_test_data.select('Id').toPandas()
result=result.set_index('Id').reindex(id_test.set_index('Id').index).reset_index()
result = result.rename(columns = {'prediction' :'winPlacePerc'})
result.to_csv('submission2.csv', index=False)
